# ğŸ¤– Jarvis AI Assistant

A personal AI assistant powered by a **self-hosted LLM** (via LM Studio) with **Pinecone vector database** for knowledge retrieval and a modern **chatbot UI**.

![Jarvis AI Assistant](screenshot.png)

---

## âœ¨ Features

- ï¿½ **Self-hosted LLM** - Uses LM Studio's OpenAI-compatible API (Qwen2.5-Coder-7B or any model)
- ğŸ“š **Knowledge Base** - Store and retrieve information using Pinecone vector database
- ğŸ’¬ **Modern Chat UI** - Clean, responsive dark-themed web interface
- ğŸ”„ **Conversation Memory** - Maintains context across messages in a session
- ğŸ” **Semantic Search** - Find relevant knowledge using sentence embeddings
- âš¡ **Real-time Status** - Live connection status for LLM and vector database

---

## ğŸ“ Project Structure

```
jarvis-assistant/
â”œâ”€â”€ app.py              # Main Flask application & API endpoints
â”œâ”€â”€ config.py           # Configuration settings (loaded from .env)
â”œâ”€â”€ llm_client.py       # LLM client for LM Studio (OpenAI-compatible)
â”œâ”€â”€ vector_db.py        # Pinecone vector database client
â”œâ”€â”€ requirements.txt    # Python dependencies
â”œâ”€â”€ .env                # Environment variables (create from .env.example)
â”œâ”€â”€ .env.example        # Example environment file
â”œâ”€â”€ README.md           # This file
â””â”€â”€ static/
    â”œâ”€â”€ index.html      # Chat UI HTML
    â”œâ”€â”€ styles.css      # Dark theme styling
    â””â”€â”€ script.js       # Frontend JavaScript
```

---

## ğŸ› ï¸ Prerequisites

### 1. LM Studio (Required)
- Download from: https://lmstudio.ai/
- Load a model (e.g., Qwen2.5-Coder-7B-Instruct)
- Start the local server (default: `http://localhost:1234`)

### 2. Pinecone Account (Optional - for Knowledge Base)
- Sign up at: https://www.pinecone.io/ (free tier available)
- Create an API key from the dashboard

### 3. Python 3.9+
- Download from: https://www.python.org/downloads/

---

## ğŸš€ Quick Start

### 1. Clone/Download the Project

```bash
cd jarvis-assistant
```

### 2. Create Virtual Environment (Recommended)

```bash
python -m venv venv

# Windows
venv\Scripts\activate

# macOS/Linux
source venv/bin/activate
```

### 3. Install Dependencies

```bash
pip install -r requirements.txt
```

### 4. Configure Environment

Copy `.env.example` to `.env` and update the values:

```env
# LM Studio Settings
LLM_BASE_URL=http://localhost:1234/v1
LLM_MODEL=qwen2.5-coder-7b-instruct

# Pinecone Settings (optional)
PINECONE_API_KEY=your-pinecone-api-key-here
PINECONE_INDEX_NAME=jarvis-knowledge

# Flask Settings
FLASK_DEBUG=False
FLASK_PORT=5000
```

### 5. Start the Application

```bash
python app.py
```

### 6. Open the Chat UI

Navigate to: **http://localhost:5000**

---

## ğŸ“– Usage Guide

### ğŸ’¬ Chat
1. Type your message in the input field
2. Press **Enter** or click the send button
3. Jarvis will respond using the LLM
4. Conversation history is maintained within the session

### ğŸ“š Knowledge Base
1. Click **"Knowledge"** in the sidebar
2. **Add Knowledge**: Enter information you want Jarvis to remember
3. **Search Knowledge**: Query your stored information
4. When chatting, Jarvis automatically searches the knowledge base for context

### Example Knowledge Entries:
- Company policies and procedures
- Product documentation
- FAQ answers
- Technical specifications
- Personal notes and reminders

---

## ğŸ”Œ API Endpoints

### Chat
```http
POST /api/chat
Content-Type: application/json

{
    "message": "Your message here",
    "session_id": "optional-session-id",
    "use_knowledge": true
}
```

### Add Knowledge
```http
POST /api/knowledge
Content-Type: application/json

{
    "text": "Information to store",
    "metadata": {"source": "optional metadata"}
}
```

### Search Knowledge
```http
POST /api/knowledge/search
Content-Type: application/json

{
    "query": "search query",
    "top_k": 3
}
```

### Health Check
```http
GET /api/health
```

### Clear Session
```http
POST /api/session/clear
Content-Type: application/json

{
    "session_id": "session-id"
}
```

---

## âš™ï¸ Configuration Options

| Variable | Description | Default |
|----------|-------------|---------|
| `LLM_BASE_URL` | LM Studio API endpoint | `http://localhost:1234/v1` |
| `LLM_MODEL` | Model identifier | `qwen2.5-coder-7b-instruct` |
| `PINECONE_API_KEY` | Pinecone API key | (empty) |
| `PINECONE_INDEX_NAME` | Pinecone index name | `jarvis-knowledge` |
| `FLASK_DEBUG` | Enable Flask debug mode | `False` |
| `FLASK_PORT` | Server port | `5000` |

---

## ğŸ”§ Troubleshooting

### LLM Not Connecting
- âœ… Ensure LM Studio is running and the server is started
- âœ… Check the `LLM_BASE_URL` in your `.env` file
- âœ… Verify a model is loaded in LM Studio
- âœ… Test the endpoint: `curl http://localhost:1234/v1/models`

### Pinecone Errors
- âœ… Verify your API key is correct
- âœ… Check your Pinecone dashboard for quota limits
- âœ… Ensure the index name doesn't contain invalid characters

### Port Already in Use
- Change `FLASK_PORT` in `.env` to a different port (e.g., 5001)

### Module Not Found Errors
- Ensure you've activated the virtual environment
- Run `pip install -r requirements.txt` again

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 â”‚     â”‚                 â”‚     â”‚                 â”‚
â”‚   Web Browser   â”‚â”€â”€â”€â”€â–¶â”‚  Flask Server   â”‚â”€â”€â”€â”€â–¶â”‚   LM Studio     â”‚
â”‚   (Chat UI)     â”‚     â”‚   (app.py)      â”‚     â”‚   (Local LLM)   â”‚
â”‚                 â”‚     â”‚                 â”‚     â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚                 â”‚
                        â”‚    Pinecone     â”‚
                        â”‚  (Vector DB)    â”‚
                        â”‚                 â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Tech Stack

- **Backend**: Python, Flask, Flask-CORS
- **LLM Client**: OpenAI Python SDK (compatible with LM Studio)
- **Vector Database**: Pinecone
- **Embeddings**: Sentence-Transformers (all-MiniLM-L6-v2)
- **Frontend**: HTML5, CSS3, Vanilla JavaScript

---

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- [LM Studio](https://lmstudio.ai/) - Local LLM inference
- [Pinecone](https://www.pinecone.io/) - Vector database
- [Sentence-Transformers](https://www.sbert.net/) - Text embeddings
- [Flask](https://flask.palletsprojects.com/) - Web framework

---

**Built with â¤ï¸ for the Diligent "Code Meets Co-Pilot" Workshop**
